{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "source_data=[]\n",
    "target_data=[]\n",
    "with open('protein-secondary-structure/2018-06-06-pdb-intersect-pisces.csv') as file:\n",
    "    data=csv.reader(file, delimiter=' ')\n",
    "    next(data, None)\n",
    "    for row in data:\n",
    "        source_data.append(row[0].split(',')[2])\n",
    "        target_data.append(row[0].split(',')[3])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef extract_character_vocab(data):\\n    set_words=set([char for seq in data for char in seq])\\n    int2vocab={i:word for i, word in enumerate(list(set_words)+[\"<PAD>\"])}\\n    vocab2int={word:i for i,word in int2vocab.items()}\\n    return int2vocab, vocab2int\\nsource_int_to_letter, source_letter_to_int = extract_character_vocab(source_data)\\n#target_int_to_letter, target_letter_to_int = extract_character_vocab(target_data)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def extract_character_vocab(data):\n",
    "    set_words=set([char for seq in data for char in seq])\n",
    "    int2vocab={i:word for i, word in enumerate(list(set_words)+[\"<PAD>\"])}\n",
    "    vocab2int={word:i for i,word in int2vocab.items()}\n",
    "    return int2vocab, vocab2int\n",
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_data)\n",
    "#target_int_to_letter, target_letter_to_int = extract_character_vocab(target_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_letter_to_int={'I': 0, 'A': 1, 'C': 2, 'V': 3, 'S': 4, 'K': 5, 'T': 6, 'P': 7, 'N': 8, 'L': 9, 'R': 10, 'W': 11, '*': 12, 'F': 13, 'G': 14, 'Q': 15, 'M': 16, 'Y': 17, 'H': 18, 'D': 19, 'E': 20,'<PAD>':21}\n",
    "source_int_to_letter={0: 'I', 1: 'A', 2: 'C', 3: 'V', 4: 'S', 5: 'K', 6: 'T', 7: 'P', 8: 'N', 9: 'L', 10: 'R', 11: 'W', 12: '*', 13: 'F', 14: 'G', 15: 'Q', 16: 'M', 17: 'Y', 18: 'H', 19: 'D', 20: 'E', 21:'<PAD>'}\n",
    "target_int_to_letter={0: 'I', 1: 'T', 2: 'E', 3: 'H', 4: 'C', 5: 'B', 6: 'S', 7: 'G', 8:'<PAD>'}\n",
    "target_letter_to_int={'I': 0, 'T': 1, 'E': 2, 'H': 3, 'C': 4, 'B': 5, 'S': 6, 'G': 7, '<PAD>':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing only\n",
    "\n",
    "import glob\n",
    "test_sor=[]\n",
    "test_tar=[]\n",
    "for file in glob.glob('CB513_RES_nd_its_Secondary_only/CB515/*'):\n",
    "    with open(file) as f:\n",
    "        data=f.readlines()\n",
    "        test_sor.append(data[0])\n",
    "        test_tar.append(data[1])\n",
    "\n",
    "        \n",
    "        \n",
    "test_sor_=[ val[4:].strip(\"\\n\") for val in test_sor ]\n",
    "test_tar_=[ val[7:].strip(\"\\n\") for val in test_tar ]\n",
    "\n",
    "test_sor_=[''.join(val.split(',')) for val in test_sor_]\n",
    "test_tar_=[''.join(val.split(',')) for val in test_tar_]\n",
    "\n",
    "test_seq_id=[[source_letter_to_int.get(letter, source_letter_to_int['*']) for letter in line] for line in test_sor_]\n",
    "test_tar_id=[[target_letter_to_int.get(letter) for letter in line] for line in test_tar_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "index=[]\n",
    "for val in test_tar_id:\n",
    "    for i in val:\n",
    "        if i==None:\n",
    "            index.append(c)\n",
    "            break\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_=sorted(index, reverse=True)\n",
    "for idx in index_:\n",
    "    del test_seq_id[idx]\n",
    "    del test_tar_id[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_id=test_seq_id[:-38]\n",
    "test_tar_id=test_tar_id[:-38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_letter_ids_ = [[source_letter_to_int[letter] for letter in line] for line in source_data]\n",
    "target_letter_ids_ = [[target_letter_to_int[letter] for letter in line] for line in target_data] \n",
    "\n",
    "source_letter_ids_=[val for val in source_letter_ids_ if len(val)<300]\n",
    "target_letter_ids_=[val for val in target_letter_ids_ if len(val)<300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_letter_ids_=np.asarray(source_letter_ids_)\n",
    "#source_letter_ids_=source_letter_ids_/21\n",
    "\n",
    "target_letter_ids_=np.asarray(target_letter_ids_)\n",
    "\n",
    "from random import shuffle\n",
    "idx=[i for i in range(len(source_letter_ids_))]\n",
    "shuffle(idx)\n",
    "source_letter_ids_=source_letter_ids_[idx,]\n",
    "target_letter_ids_=target_letter_ids_[idx,]\n",
    "\n",
    "source_letter_ids=[]\n",
    "target_letter_ids=[]\n",
    "for val in source_letter_ids_:source_letter_ids.append(val)\n",
    "for val in target_letter_ids_:target_letter_ids.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_letter_ids=source_letter_ids[:-12]\n",
    "target_letter_ids=target_letter_ids[:-12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=25\n",
    "lstm_units=32\n",
    "num_layers=2\n",
    "batch_size=32\n",
    "learn_rate=1e-3\n",
    "#embed_dim=50\n",
    "display_step=100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, ?, 50), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"concat_4:0\", shape=(?, ?, 272), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"concat_5:0\", shape=(32, ?, 64), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph=tf.Graph()\n",
    "\n",
    "def makeCell(lstm_units):\n",
    "    cell=tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(lstm_units, initializer=tf.random_uniform_initializer(-1, 1, seed=2)), output_keep_prob=0.75)\n",
    "    return cell\n",
    "\n",
    "with train_graph.as_default():\n",
    "    with tf.device('/gpu:0'):\n",
    "        x=tf.placeholder(tf.int32, [None,None])\n",
    "        y=tf.placeholder(tf.int32, [None,None])\n",
    "        lr=tf.placeholder(tf.float32)\n",
    "        y_onehot=tf.one_hot(y, len(target_letter_to_int))\n",
    "        #x_onehot=tf.one_hot(x, len(source_letter_to_int))\n",
    "        #rev_x=tf.reverse(x, [-1])\n",
    "        embed=tf.contrib.layers.embed_sequence(x, len(source_letter_to_int), 50)\n",
    "        print(embed)\n",
    "\n",
    "        conv3_1=tf.layers.conv1d(embed,8, 3, padding='same')\n",
    "        conv3_2=tf.layers.conv1d(conv3_1,16, 3, padding='same')\n",
    "        conv3_2=tf.nn.dropout(conv3_2, keep_prob=0.65)\n",
    "        conv3_3=tf.layers.conv1d(conv3_2,32, 3, padding='same')\n",
    "        conv3=tf.nn.dropout(conv3_3, keep_prob=0.7)\n",
    "        \n",
    "        conv5_1=tf.layers.conv1d(embed,8, 5, padding='same')\n",
    "        conv5_2=tf.layers.conv1d(conv5_1,16, 5, padding='same')\n",
    "        conv5_2=tf.nn.dropout(conv5_2, keep_prob=0.65)\n",
    "        conv5_3=tf.layers.conv1d(conv5_2, 32, 5, padding='same')\n",
    "        conv5_3=tf.nn.dropout(conv5_3, keep_prob=0.7)\n",
    "        conv5_4=tf.layers.conv1d(embed,8, 2, padding='same')\n",
    "        conv5_5=tf.layers.conv1d(conv5_4, 16, 2, padding='same')\n",
    "        conv5_5=tf.nn.dropout(conv5_5, keep_prob=0.6)\n",
    "        conv5=tf.concat([conv5_3, conv5_5], 2)\n",
    "\n",
    "        conv7_1=tf.layers.conv1d(embed,8, 7, padding='same')\n",
    "        conv7_2=tf.layers.conv1d(conv7_1,16, 7, padding='same')\n",
    "        conv7_2=tf.nn.dropout(conv7_2, keep_prob=0.65)\n",
    "        conv7_3=tf.layers.conv1d(conv7_2, 32, 7, padding='same')\n",
    "        conv7_3=tf.nn.dropout(conv7_3, keep_prob=0.7)\n",
    "        conv7_4=tf.layers.conv1d(embed,8, 3, padding='same')\n",
    "        conv7_5=tf.layers.conv1d(conv7_4, 16, 3, padding='same')\n",
    "        conv7_6=tf.layers.conv1d(conv7_5, 32, 3, padding='same')\n",
    "        conv7_6=tf.nn.dropout(conv7_6, keep_prob=0.65)\n",
    "        conv7=tf.concat([conv7_3, conv7_6], 2)\n",
    "\n",
    "        conv9_1=tf.layers.conv1d(embed,8, 9, padding='same')\n",
    "        conv9_2=tf.layers.conv1d(conv9_1,16, 9, padding='same')\n",
    "        conv9_2=tf.nn.dropout(conv9_2, keep_prob=0.65)\n",
    "        conv9_3=tf.layers.conv1d(conv9_2, 32, 9, padding='same')\n",
    "        conv9_3=tf.nn.dropout(conv9_3, keep_prob=0.7)\n",
    "        conv9_4=tf.layers.conv1d(embed,8, 3, padding='same')\n",
    "        conv9_5=tf.layers.conv1d(conv9_4, 16, 3, padding='same')\n",
    "        conv9_5=tf.nn.dropout(conv9_5, keep_prob=0.65)\n",
    "        conv9_6=tf.layers.conv1d(conv9_5, 32, 3, padding='same')\n",
    "        conv9_7=tf.layers.conv1d(conv9_6, 32, 3, padding='same')\n",
    "        conv9_7=tf.nn.dropout(conv9_7, keep_prob=0.6)\n",
    "        conv9=tf.concat([conv9_3, conv9_7], 2)\n",
    "\n",
    "        conv11_1=tf.layers.conv1d(embed,8, 11, padding='same')\n",
    "        conv11_2=tf.layers.conv1d(conv11_1,16, 11, padding='same')\n",
    "        conv11_2=tf.nn.dropout(conv11_2, keep_prob=0.65)\n",
    "        conv11_3=tf.layers.conv1d(conv11_2, 32, 11, padding='same')\n",
    "        conv11_3=tf.nn.dropout(conv11_3, keep_prob=0.7)\n",
    "        conv11_4=tf.layers.conv1d(embed,8, 3, padding='same')\n",
    "        conv11_5=tf.layers.conv1d(conv11_4, 16, 3, padding='same')\n",
    "        conv11_6=tf.layers.conv1d(conv11_5, 16, 3, padding='same')\n",
    "        conv11_6=tf.nn.dropout(conv11_6, keep_prob=0.65)\n",
    "        conv11_7=tf.layers.conv1d(conv11_6, 32, 3, padding='same')\n",
    "        conv11_8=tf.layers.conv1d(conv11_7, 32, 3, padding='same')\n",
    "        conv11_8=tf.nn.dropout(conv11_8, keep_prob=0.5)\n",
    "        conv11=tf.concat([conv11_3, conv11_8], 2)\n",
    "   \n",
    "        conv_concat=tf.concat([conv3, conv5, conv7, conv9, conv11], 2)\n",
    "        print(conv_concat)\n",
    "        \n",
    "        fw_cell=tf.contrib.rnn.MultiRNNCell([makeCell(lstm_units) for _ in range(num_layers)])\n",
    "        bw_cell=tf.contrib.rnn.MultiRNNCell([makeCell(lstm_units) for _ in range(num_layers)])\n",
    "\n",
    "        fw_initial_state=fw_cell.zero_state(batch_size, tf.float32)\n",
    "        bw_initial_state=bw_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "  \n",
    "        out, final_state=tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, conv_concat, initial_state_fw=fw_initial_state, initial_state_bw=bw_initial_state)\n",
    "        out=tf.concat(out, 2)\n",
    "        print(out)\n",
    "\n",
    "    logits=tf.contrib.layers.fully_connected(out,len(target_letter_to_int), activation_fn=tf.nn.softmax)\n",
    "    with tf.device('/gpu:0'):\n",
    "        loss=tf.reduce_mean(tf.losses.log_loss(predictions=logits, labels=y_onehot))\n",
    "        optm=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "        \n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    model_vars=tf.trainable_variables()\n",
    "  \n",
    "    saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "EmbedSequence/embeddings:0 (float32_ref 22x50) [1100, bytes: 4400]\n",
      "conv1d/kernel:0 (float32_ref 3x50x8) [1200, bytes: 4800]\n",
      "conv1d/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_1/kernel:0 (float32_ref 3x8x16) [384, bytes: 1536]\n",
      "conv1d_1/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_2/kernel:0 (float32_ref 3x16x32) [1536, bytes: 6144]\n",
      "conv1d_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_3/kernel:0 (float32_ref 5x50x8) [2000, bytes: 8000]\n",
      "conv1d_3/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_4/kernel:0 (float32_ref 5x8x16) [640, bytes: 2560]\n",
      "conv1d_4/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_5/kernel:0 (float32_ref 5x16x32) [2560, bytes: 10240]\n",
      "conv1d_5/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_6/kernel:0 (float32_ref 2x50x8) [800, bytes: 3200]\n",
      "conv1d_6/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_7/kernel:0 (float32_ref 2x8x16) [256, bytes: 1024]\n",
      "conv1d_7/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_8/kernel:0 (float32_ref 7x50x8) [2800, bytes: 11200]\n",
      "conv1d_8/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_9/kernel:0 (float32_ref 7x8x16) [896, bytes: 3584]\n",
      "conv1d_9/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_10/kernel:0 (float32_ref 7x16x32) [3584, bytes: 14336]\n",
      "conv1d_10/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_11/kernel:0 (float32_ref 3x50x8) [1200, bytes: 4800]\n",
      "conv1d_11/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_12/kernel:0 (float32_ref 3x8x16) [384, bytes: 1536]\n",
      "conv1d_12/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_13/kernel:0 (float32_ref 3x16x32) [1536, bytes: 6144]\n",
      "conv1d_13/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_14/kernel:0 (float32_ref 9x50x8) [3600, bytes: 14400]\n",
      "conv1d_14/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_15/kernel:0 (float32_ref 9x8x16) [1152, bytes: 4608]\n",
      "conv1d_15/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_16/kernel:0 (float32_ref 9x16x32) [4608, bytes: 18432]\n",
      "conv1d_16/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_17/kernel:0 (float32_ref 3x50x8) [1200, bytes: 4800]\n",
      "conv1d_17/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_18/kernel:0 (float32_ref 3x8x16) [384, bytes: 1536]\n",
      "conv1d_18/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_19/kernel:0 (float32_ref 3x16x32) [1536, bytes: 6144]\n",
      "conv1d_19/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_20/kernel:0 (float32_ref 3x32x32) [3072, bytes: 12288]\n",
      "conv1d_20/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_21/kernel:0 (float32_ref 11x50x8) [4400, bytes: 17600]\n",
      "conv1d_21/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_22/kernel:0 (float32_ref 11x8x16) [1408, bytes: 5632]\n",
      "conv1d_22/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_23/kernel:0 (float32_ref 11x16x32) [5632, bytes: 22528]\n",
      "conv1d_23/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_24/kernel:0 (float32_ref 3x50x8) [1200, bytes: 4800]\n",
      "conv1d_24/bias:0 (float32_ref 8) [8, bytes: 32]\n",
      "conv1d_25/kernel:0 (float32_ref 3x8x16) [384, bytes: 1536]\n",
      "conv1d_25/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_26/kernel:0 (float32_ref 3x16x16) [768, bytes: 3072]\n",
      "conv1d_26/bias:0 (float32_ref 16) [16, bytes: 64]\n",
      "conv1d_27/kernel:0 (float32_ref 3x16x32) [1536, bytes: 6144]\n",
      "conv1d_27/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "conv1d_28/kernel:0 (float32_ref 3x32x32) [3072, bytes: 12288]\n",
      "conv1d_28/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0 (float32_ref 304x128) [38912, bytes: 155648]\n",
      "bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel:0 (float32_ref 64x128) [8192, bytes: 32768]\n",
      "bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0 (float32_ref 304x128) [38912, bytes: 155648]\n",
      "bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel:0 (float32_ref 64x128) [8192, bytes: 32768]\n",
      "bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "fully_connected/weights:0 (float32_ref 64x9) [576, bytes: 2304]\n",
      "fully_connected/biases:0 (float32_ref 9) [9, bytes: 36]\n",
      "Total size of variables: 150685\n",
      "Total bytes of variables: 602740\n"
     ]
    }
   ],
   "source": [
    "def get_summary(vars_):\n",
    "    tf.contrib.slim.model_analyzer.analyze_vars(vars_, print_info=True)\n",
    "    \n",
    "get_summary(model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentences, pad_int_id):\n",
    "    max_len=max([len(sentence) for sentence in sentences])\n",
    "    return [sentence + [pad_int_id]*(max_len-len(sentence)) for sentence in sentences]\n",
    "\n",
    "def get_batches(sources, targets, source_pad_int, target_pad_int, batch_size):\n",
    "    \n",
    "    for i in range(0, len(sources),batch_size):\n",
    "        \n",
    "        end=i+batch_size\n",
    "        source_batch=sources[i:end]\n",
    "        target_batch=targets[i:end]\n",
    "        source_batch_pad=np.array(pad_sentence_batch(source_batch, source_pad_int))\n",
    "        #source_batch_pad=source_batch_pad/21\n",
    "        #source_batch_pad=[2.*(a - np.min(a))/np.ptp(a)-1 for a in source_batch_pad]\n",
    "        #source_batch_pad=[arr[:, np.newaxis] for arr in source_batch_pad]\n",
    "        target_batch_pad=np.array(pad_sentence_batch(target_batch, target_pad_int))\n",
    "\n",
    "        yield source_batch_pad, target_batch_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_source, valid_target=next(get_batches(source_letter_ids, target_letter_ids, source_letter_to_int['<PAD>'], target_letter_to_int['<PAD>'],batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 236)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(valid_source).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0/25   Batch   0/200   loss:  0.376  val_loss 0.328\n",
      "Epoch  0/25   Batch 100/200   loss:  0.166  val_loss 0.133\n",
      "Epoch  1/25   Batch   0/200   loss:  0.121  val_loss 0.122\n",
      "Epoch  1/25   Batch 100/200   loss:  0.145  val_loss 0.118\n",
      "Epoch  2/25   Batch   0/200   loss:  0.113  val_loss 0.113\n",
      "Epoch  2/25   Batch 100/200   loss:  0.137  val_loss 0.111\n",
      "Epoch  3/25   Batch   0/200   loss:  0.109  val_loss 0.108\n",
      "Epoch  3/25   Batch 100/200   loss:  0.135  val_loss 0.107\n",
      "Epoch  4/25   Batch   0/200   loss:  0.107  val_loss 0.105\n",
      "Epoch  4/25   Batch 100/200   loss:  0.131  val_loss 0.105\n",
      "Epoch  5/25   Batch   0/200   loss:  0.103  val_loss 0.102\n",
      "Epoch  5/25   Batch 100/200   loss:  0.126  val_loss 0.101\n",
      "Epoch  6/25   Batch   0/200   loss:  0.099  val_loss 0.099\n",
      "Epoch  6/25   Batch 100/200   loss:  0.124  val_loss 0.099\n",
      "Epoch  7/25   Batch   0/200   loss:  0.099  val_loss 0.098\n",
      "Epoch  7/25   Batch 100/200   loss:  0.124  val_loss 0.098\n",
      "Epoch  8/25   Batch   0/200   loss:  0.099  val_loss 0.097\n",
      "Epoch  8/25   Batch 100/200   loss:  0.123  val_loss 0.097\n",
      "Epoch  9/25   Batch   0/200   loss:  0.096  val_loss 0.097\n",
      "Epoch  9/25   Batch 100/200   loss:  0.121  val_loss 0.095\n",
      "Epoch 10/25   Batch   0/200   loss:  0.097  val_loss 0.096\n",
      "Epoch 10/25   Batch 100/200   loss:  0.122  val_loss 0.096\n",
      "Epoch 11/25   Batch   0/200   loss:  0.095  val_loss 0.095\n",
      "Epoch 11/25   Batch 100/200   loss:  0.120  val_loss 0.094\n",
      "Epoch 12/25   Batch   0/200   loss:  0.095  val_loss 0.096\n",
      "Epoch 12/25   Batch 100/200   loss:  0.121  val_loss 0.093\n",
      "Epoch 13/25   Batch   0/200   loss:  0.096  val_loss 0.096\n",
      "Epoch 13/25   Batch 100/200   loss:  0.119  val_loss 0.093\n",
      "Epoch 14/25   Batch   0/200   loss:  0.094  val_loss 0.094\n",
      "Epoch 14/25   Batch 100/200   loss:  0.121  val_loss 0.093\n",
      "Epoch 15/25   Batch   0/200   loss:  0.094  val_loss 0.095\n",
      "Epoch 15/25   Batch 100/200   loss:  0.119  val_loss 0.093\n",
      "Epoch 16/25   Batch   0/200   loss:  0.093  val_loss 0.093\n",
      "Epoch 16/25   Batch 100/200   loss:  0.120  val_loss 0.093\n",
      "Epoch 17/25   Batch   0/200   loss:  0.094  val_loss 0.093\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-ee741e0d7126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_bw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtrain_writer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer1=tf.summary.FileWriter('./summary/train_loss', train_graph)\n",
    "    train_writer2=tf.summary.FileWriter('./summary/valid_loss', train_graph)\n",
    "    merge=tf.summary.merge_all()\n",
    "    counter=0\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        state_fw=sess.run(fw_initial_state)\n",
    "        state_bw=sess.run(bw_initial_state)\n",
    "        \n",
    "        for batch_i, (train_source, train_target) in enumerate(get_batches(source_letter_ids, target_letter_ids, source_letter_to_int['<PAD>'], target_letter_to_int['<PAD>'], batch_size)):\n",
    "        \n",
    "        \n",
    "            feed={x:train_source,\n",
    "                 y:train_target,\n",
    "                 lr:learn_rate,\n",
    "                 fw_initial_state:state_fw,\n",
    "                 bw_initial_state:state_bw}\n",
    "            \n",
    "            counter+=1\n",
    "        \n",
    "            summary, cost, (state_fw, state_bw), _= sess.run([merge, loss, final_state, optm], feed)\n",
    "            \n",
    "            train_writer1.add_summary(summary, counter)\n",
    "        \n",
    "            if batch_i%display_step==0:\n",
    "                \n",
    "                feed={x:valid_source,\n",
    "                     y:valid_target}\n",
    "\n",
    "                summary, valid_loss=sess.run([merge, loss], feed)\n",
    "                train_writer2.add_summary(summary, counter)\n",
    "                \n",
    "                print(\"Epoch{:>3}/{}   Batch{:>4}/{}   loss: {:>6.3f}  val_loss{:>6.3f}\".format(\n",
    "                                                epoch,\n",
    "                                                epochs,\n",
    "                                                batch_i,\n",
    "                                                len(source_letter_ids)//batch_size,\n",
    "                                                cost,\n",
    "                                                valid_loss))\n",
    "        \n",
    "        saver.save(sess, \"checkpoint/trained_weights12.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"w10 0.133/0.119 without inter connection\"\n",
    "\"w11 0.116/0.111 in epoch 15/25, 2layer 1 dropout 1 layer 1 dropout-> embed 1 layer16F - 32F dropout of 0.6 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples=test_seq_id[0]\n",
    "targets=test_tar_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples=source_letter_ids[9]\n",
    "targets=target_letter_ids[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(sources, targets):\n",
    "    for i in range(0, len(sources)):\n",
    "        yield sources[i], targets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/trained_weights12.ckpt\n",
      "\n",
      "\n",
      "Sequence Length :  107 \n",
      "\n",
      "Actual Output Sequence :  CHHHHHHHHHHHTTTGGGTTTCTTTTTTTCCCTTTTCCTTTTHHHHHHHHHHHHHHHHCCCHHHHHHHHHHHHTTTCHHHHHHHHHHHHHHCGGGTTTCCCCHHHHC \n",
      "\n",
      "[4, 4, 4, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 4, 4, 4, 4, 1, 1, 4, 2, 2, 2, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 1, 1, 4, 1, 1, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3]\n",
      "Generated Output Sequence :  CCCHHEHEEHHHHHHHHHHCCHHHHHHHTCCCCTTCEEEECCHHHHHHHHHHHHHHHHHTCHHHHHHHHHHHHHHHHHTCHHHHHHEEEEEHTTCTTCCCCCHHHHH \n",
      "\n",
      "Output seq length :  107 \n",
      "\n",
      "Prediction Score :  55.14018535614014\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoint/'))\n",
    "    print(\"\\n\")\n",
    "    predict=sess.run(logits, {x:[samples]*batch_size})\n",
    "    #print(\"Input Sequence : \", test_sor_[1], \"\\n\")\n",
    "    print(\"Sequence Length : \", len(samples), \"\\n\")\n",
    "    print(\"Actual Output Sequence : \", test_tar_[0], \"\\n\")\n",
    "    act=[sess.run(tf.argmax(val, axis=0)) for val in predict[0] ]\n",
    "    print(act)\n",
    "    score=sess.run(tf.reduce_mean(tf.cast(tf.equal(act, targets), tf.float32)))*100\n",
    "    output_seq=\"\".join(target_int_to_letter[i] for i in act if i != target_letter_to_int['<PAD>'])\n",
    "    print(\"Generated Output Sequence : \", output_seq, \"\\n\")\n",
    "    print(\"Output seq length : \", len(output_seq), \"\\n\")\n",
    "    print(\"Prediction Score : \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/trained_weights12.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/trained_weights12.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[0.767214, 0.7962677]\n",
      "Tensor(\"mul:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#checking complete CB513\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoint/'))\n",
    "    print(\"\\n\")\n",
    "    score=[]\n",
    "    for samples, targets in get_batches(test_seq_id, test_tar_id, source_letter_to_int['<PAD>'], target_letter_to_int['<PAD>'],batch_size):\n",
    "        act=[]\n",
    "        predict=sess.run(logits, {x:samples})\n",
    "        act.append([sess.run([tf.argmax(val, axis=0) for val in values]) for values in predict])\n",
    "        act=np.array(act)\n",
    "        act=act[0, :, :]\n",
    "        targets=np.array(targets)\n",
    "        score.append(sess.run(tf.reduce_mean(tf.cast(tf.equal(act, targets), tf.float32))))\n",
    "    final_score=sess.run(tf.reduce_mean(score))*100\n",
    "    print(score)\n",
    "    print(\"Test Acc: \", final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   print(\"\\n\")\n",
    "    print(act, \"\\t\", len(act))\n",
    "    print(\"\\n\")\n",
    "    output_seq=\"\".join(target_int_to_letter[i] for i in act if i != target_letter_to_int['<PAD>'])\n",
    "    print(output_seq, \"\\t\", len(output_seq))\n",
    "    print(\"\\n\")\n",
    "    print(\"Prediction Score : \", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
